{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJIBjyHI81gRl+sf0AAcBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliwadler/image_classification/blob/main/cat_dog_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q1fXlMeMW1YO",
        "outputId": "da275197-d18a-4d32-bf27-040d233f14b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('cats_dogs_classifier.h5', <http.client.HTTPMessage at 0x7f8b86215250>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Download the dataset zip file\n",
        "url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
        "filename = \"cats_and_dogs_filtered.zip\"\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1HZZ9Kw4s8dZA9jN_J0En3p0a41pW2kEX/view?usp=sharing\"\n",
        "filename = \"cats_dogs_classifier.h5\"\n",
        "urllib.request.urlretrieve(url, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "fd7hDq8KXUed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'cats_and_dogs_filtered/train'\n",
        "validation_dir = 'cats_and_dogs_filtered/validation'\n",
        "\n",
        "# TensorFlow - Klasse: erstellen einer Pipeline für das Lesen von Bildern aus Verzeichnis, Bilder in Echtzeit währen des Trainings verändern (Drehung, Zuschneiden, Spiegeln, ...)\n",
        "# Erleichtert das Training von Deep-Learning-Modellen mit Bildern, da sie eine effiziente Möglichkeit bietet, eine große Anzahl von Bildern automatisch zu verarbeiten und zu augmentieren.\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "gtSQBahSXYrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4Pp1JypXbJ7",
        "outputId": "089699a2-7b4b-4970-8b1a-0c0a08e50621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "oTveRchwXfxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PNL6_-OrXguw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Y4cVxuXujX",
        "outputId": "ee325789-ef6f-49ac-9de8-3d9bf4491d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 137s 1s/step - loss: 0.6894 - accuracy: 0.5410 - val_loss: 0.6844 - val_accuracy: 0.5490\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.6731 - accuracy: 0.5910 - val_loss: 0.6570 - val_accuracy: 0.5980\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 141s 1s/step - loss: 0.6520 - accuracy: 0.6200 - val_loss: 0.6875 - val_accuracy: 0.5700\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.6276 - accuracy: 0.6445 - val_loss: 0.6149 - val_accuracy: 0.6650\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 138s 1s/step - loss: 0.5917 - accuracy: 0.6970 - val_loss: 0.5841 - val_accuracy: 0.6970\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 133s 1s/step - loss: 0.5547 - accuracy: 0.7135 - val_loss: 0.5857 - val_accuracy: 0.6950\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 139s 1s/step - loss: 0.5393 - accuracy: 0.7195 - val_loss: 0.5630 - val_accuracy: 0.7160\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.5168 - accuracy: 0.7470 - val_loss: 0.6416 - val_accuracy: 0.6600\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 132s 1s/step - loss: 0.4955 - accuracy: 0.7510 - val_loss: 0.5486 - val_accuracy: 0.7170\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.4760 - accuracy: 0.7790 - val_loss: 0.5556 - val_accuracy: 0.7170\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 132s 1s/step - loss: 0.4563 - accuracy: 0.7805 - val_loss: 0.5560 - val_accuracy: 0.7190\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 140s 1s/step - loss: 0.4358 - accuracy: 0.7930 - val_loss: 0.5539 - val_accuracy: 0.7300\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.4146 - accuracy: 0.8125 - val_loss: 0.5939 - val_accuracy: 0.7080\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.3954 - accuracy: 0.8190 - val_loss: 0.5425 - val_accuracy: 0.7350\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 137s 1s/step - loss: 0.3776 - accuracy: 0.8355 - val_loss: 0.6080 - val_accuracy: 0.7160\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 141s 1s/step - loss: 0.3537 - accuracy: 0.8340 - val_loss: 0.5208 - val_accuracy: 0.7420\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.3298 - accuracy: 0.8585 - val_loss: 0.5323 - val_accuracy: 0.7420\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 137s 1s/step - loss: 0.3192 - accuracy: 0.8635 - val_loss: 0.5480 - val_accuracy: 0.7420\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.2924 - accuracy: 0.8820 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.2742 - accuracy: 0.8895 - val_loss: 0.5490 - val_accuracy: 0.7540\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.2584 - accuracy: 0.9000 - val_loss: 0.6174 - val_accuracy: 0.7180\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.2307 - accuracy: 0.9135 - val_loss: 0.5946 - val_accuracy: 0.7380\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 132s 1s/step - loss: 0.2214 - accuracy: 0.9135 - val_loss: 0.5939 - val_accuracy: 0.7350\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 138s 1s/step - loss: 0.1961 - accuracy: 0.9270 - val_loss: 0.5660 - val_accuracy: 0.7660\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.1691 - accuracy: 0.9395 - val_loss: 0.7871 - val_accuracy: 0.7100\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.1639 - accuracy: 0.9390 - val_loss: 0.6219 - val_accuracy: 0.7480\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.1518 - accuracy: 0.9475 - val_loss: 0.6141 - val_accuracy: 0.7490\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 133s 1s/step - loss: 0.1270 - accuracy: 0.9615 - val_loss: 0.6301 - val_accuracy: 0.7620\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 137s 1s/step - loss: 0.1128 - accuracy: 0.9635 - val_loss: 0.6625 - val_accuracy: 0.7580\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 133s 1s/step - loss: 0.1076 - accuracy: 0.9650 - val_loss: 0.7867 - val_accuracy: 0.7210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "history_df = pd.DataFrame(model.history.history)\n",
        "history_df.to_csv('train_history.csv', index=False)\n",
        "model.save('cats_dogs_classifier.h5')"
      ],
      "metadata": {
        "id": "3U7vm1aSgPls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('drive/MyDrive/cats_dogs_classifier.h5')\n",
        "\n",
        "# Load the image and resize to the target size\n",
        "img_path = '/content/cats_and_dogs_filtered/validation/dogs/dog.2004.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "# Convert the image to a numpy array and normalize\n",
        "img_array = image.img_to_array(img)\n",
        "img_array /= 255.\n",
        "\n",
        "# Expand the dimensions of the image array to match the model input shape\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict the class of the image\n",
        "pred = model.predict(img_array)\n",
        "pred_probability = pred[0][0]\n",
        "\n",
        "if pred[0][0] > 0.5:\n",
        "    print(\"The image is a dog with a probability of {:.2f}%\".format(pred_probability * 100))\n",
        "else:\n",
        "    print(\"The image is a cat with a probability of {:.2f}%\".format(pred_probability * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SZujwhbgWF9",
        "outputId": "2cc728ab-5aef-4f00-be1c-86c5fc7c327b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 120ms/step\n",
            "The image is a dog with a probability of 99.96%\n"
          ]
        }
      ]
    }
  ]
}